{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Briefing",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauberson/hello-world/blob/master/Briefing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZHS6Y0wT5Pq",
        "colab_type": "text"
      },
      "source": [
        "Para realiza o briefing eu optei usar o Word2Vec, é uma técnica de processamento de linguagem natural(NLP) que tem a ideia de transformar uma palavra em um vetor númerico que consiga o representar semanticamente, cada palavra tem uma unica representação. Para fazer essa representação podemos usar o encoding, porem esse metodo não leva em consideração a similaridade entre as palavras. Mas temos uma solução para isso, embedding; Esse metodo consegue considerar a similaridade entre as palavras. Word2Vec usa o metodo de embedding e a similaridade entre as palavras vem atraves de palavras \"vizinhas\". Então para prosseguir com esse metodo, o nosso banco de dados foi preenchido com manchetes. \n",
        "Dado as definições acimas, usamos o Skip Gram para definir esses \"vizinhos\", para essa tecnica foi necessario definir um raio de vizinhança, conhecido como windows size, que no desafio foi definido como 10. Não foi necessario usar uma função para remover stop words, o banco de dados ja vem com essa classe de palavras removidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o8l93q-mKhTJ",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "data = pd.read_csv(\"dados.csv\") \n",
        "data = data['manchetes'].values #data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64cp8KjTYbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "palavras = []\n",
        "for texto in data:\n",
        "  for palavra in texto.split(' '):\n",
        "      palavras.append(palavra)\n",
        "\n",
        "palavras = set(palavras) #separando cada palavra do dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcuu8PlJVdtD",
        "colab_type": "text"
      },
      "source": [
        "Skip Gram em ação! windows size = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2qp1n_HVUnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2int = {}\n",
        "\n",
        "for i,palavra in enumerate(palavras):\n",
        "    word2int[palavra] = i\n",
        "\n",
        "sentencas = []\n",
        "for sentenca in data:\n",
        "    sentencas.append(sentenca.split())\n",
        "    \n",
        "WINDOW_SIZE = 10\n",
        "\n",
        "data2 = []\n",
        "for sentenca in sentencas:\n",
        "    for idx, palavra in enumerate(sentenca):\n",
        "        for vizinho in sentenca[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentenca)) + 1] : \n",
        "            if vizinho != palavra:\n",
        "                data2.append([palavra, vizinho]) #construindo a vizinha de cada palavra, num raio de 10, windows_size."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuqkWCH1WVqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data2, columns = ['palavra', 'vizinho'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLZrAItiWW6r",
        "colab_type": "code",
        "outputId": "6e589bef-14d1-46de-9cdc-843e2c24d7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df.head(10) #previa das palavras e seus vizinhos"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>palavra</th>\n",
              "      <th>vizinho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>diretor</td>\n",
              "      <td>petrobras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>diretor</td>\n",
              "      <td>nega</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diretor</td>\n",
              "      <td>organização</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>diretor</td>\n",
              "      <td>criminosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>diretor</td>\n",
              "      <td>estatal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>diretor</td>\n",
              "      <td>notícias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>diretor</td>\n",
              "      <td>brasil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>petrobras</td>\n",
              "      <td>diretor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>petrobras</td>\n",
              "      <td>nega</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>petrobras</td>\n",
              "      <td>organização</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     palavra      vizinho\n",
              "0    diretor    petrobras\n",
              "1    diretor         nega\n",
              "2    diretor  organização\n",
              "3    diretor    criminosa\n",
              "4    diretor      estatal\n",
              "5    diretor     notícias\n",
              "6    diretor       brasil\n",
              "7  petrobras      diretor\n",
              "8  petrobras         nega\n",
              "9  petrobras  organização"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm3MsnpMYaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "ONE_HOT_DIM = len(palavras)\n",
        "\n",
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot_encoding(data_point_index):\n",
        "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
        "    one_hot_encoding[data_point_index] = 1\n",
        "    return one_hot_encoding\n",
        "\n",
        "X = [] # input word\n",
        "Y = [] # target word\n",
        "\n",
        "for x, y in zip(df['palavra'], df['vizinho']):\n",
        "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
        "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
        "\n",
        "X_train = np.asarray(X)\n",
        "Y_train = np.asarray(Y)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "\n",
        "EMBEDDING_DIM = 16\n",
        "\n",
        "# hidden layer: which represents word vector eventually\n",
        "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
        "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
        "\n",
        "# output layer\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
        "b2 = tf.Variable(tf.random_normal([1]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
        "\n",
        "# loss function: cross entropy\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
        "\n",
        "# training operation\n",
        "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TTsqbrLZOdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ca81ce62-55d3-4774-d74d-61d34702d9f6"
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) \n",
        "\n",
        "iteration = 3000\n",
        "for i in range(iteration):\n",
        "    # input is X_train which is one hot encoded word\n",
        "    # label is Y_train which is one hot encoded neighbor word\n",
        "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
        "    if i % 300 == 0:\n",
        "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 loss is :  14.70251\n",
            "iteration 300 loss is :  13.804077\n",
            "iteration 600 loss is :  13.267253\n",
            "iteration 900 loss is :  12.858142\n",
            "iteration 1200 loss is :  12.527161\n",
            "iteration 1500 loss is :  12.248286\n",
            "iteration 1800 loss is :  12.006498\n",
            "iteration 2100 loss is :  11.792816\n",
            "iteration 2400 loss is :  11.601514\n",
            "iteration 2700 loss is :  11.428628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa7PARWCZYFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f37bcfbd-0259-49f8-a556-eb8c70f85683"
      },
      "source": [
        "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
        "vectors = sess.run(W1 + b1)\n",
        "linhas = len(vectors)\n",
        "colunas = len(vectors[0])\n",
        "vectors\n",
        "\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.255359  , -0.52080166, -0.50276345, ..., -1.1171277 ,\n",
              "         0.65750355, -0.4551038 ],\n",
              "       [-0.06646907, -0.60671633,  0.6570565 , ...,  1.3313273 ,\n",
              "        -1.361109  , -0.7637092 ],\n",
              "       [-0.11208901, -0.01965748, -0.01901527, ...,  0.31761658,\n",
              "        -0.31216806,  0.46359956],\n",
              "       ...,\n",
              "       [-0.49521172,  0.49478543, -0.60808086, ..., -1.0510644 ,\n",
              "         0.12885165,  0.54251754],\n",
              "       [-1.1309541 ,  0.26152104, -0.08922659, ...,  1.2928083 ,\n",
              "         1.1811688 , -0.49136633],\n",
              "       [-1.3589094 , -0.80418366, -1.1735821 , ...,  1.0300416 ,\n",
              "        -1.1887814 , -1.264911  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kur2iZSphUfd",
        "colab_type": "text"
      },
      "source": [
        "Construção dos vetores associados a cada palavra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZH_m2IT4VrC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "9dfff5d9-b73d-4157-9d90-c60f490f7d0a"
      },
      "source": [
        "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16'])\n",
        "w2v_df['palavra'] = palavras\n",
        "w2v_df = w2v_df[['palavra', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16']]\n",
        "w2v_df\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>palavra</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>1.255359</td>\n",
              "      <td>-0.520802</td>\n",
              "      <td>-0.502763</td>\n",
              "      <td>0.981672</td>\n",
              "      <td>-0.341696</td>\n",
              "      <td>-0.364461</td>\n",
              "      <td>-0.536677</td>\n",
              "      <td>1.344214</td>\n",
              "      <td>-0.004991</td>\n",
              "      <td>0.461131</td>\n",
              "      <td>2.292317</td>\n",
              "      <td>0.870123</td>\n",
              "      <td>1.296588</td>\n",
              "      <td>-1.117128</td>\n",
              "      <td>0.657504</td>\n",
              "      <td>-0.455104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>reter</td>\n",
              "      <td>-0.066469</td>\n",
              "      <td>-0.606716</td>\n",
              "      <td>0.657057</td>\n",
              "      <td>1.960007</td>\n",
              "      <td>-2.982978</td>\n",
              "      <td>0.523689</td>\n",
              "      <td>0.150464</td>\n",
              "      <td>1.163507</td>\n",
              "      <td>-2.365527</td>\n",
              "      <td>0.605053</td>\n",
              "      <td>1.009219</td>\n",
              "      <td>-1.964705</td>\n",
              "      <td>0.086884</td>\n",
              "      <td>1.331327</td>\n",
              "      <td>-1.361109</td>\n",
              "      <td>-0.763709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>infomoney</td>\n",
              "      <td>-0.112089</td>\n",
              "      <td>-0.019657</td>\n",
              "      <td>-0.019015</td>\n",
              "      <td>-0.470011</td>\n",
              "      <td>0.099113</td>\n",
              "      <td>-0.367444</td>\n",
              "      <td>0.047196</td>\n",
              "      <td>-0.683762</td>\n",
              "      <td>-0.262133</td>\n",
              "      <td>0.319313</td>\n",
              "      <td>-0.039996</td>\n",
              "      <td>-0.298571</td>\n",
              "      <td>0.094526</td>\n",
              "      <td>0.317617</td>\n",
              "      <td>-0.312168</td>\n",
              "      <td>0.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cair</td>\n",
              "      <td>-0.276426</td>\n",
              "      <td>0.004193</td>\n",
              "      <td>-1.970239</td>\n",
              "      <td>0.632414</td>\n",
              "      <td>1.483052</td>\n",
              "      <td>-0.317908</td>\n",
              "      <td>-0.797483</td>\n",
              "      <td>-0.164617</td>\n",
              "      <td>-0.419488</td>\n",
              "      <td>-0.272668</td>\n",
              "      <td>-0.280760</td>\n",
              "      <td>-1.772191</td>\n",
              "      <td>0.971081</td>\n",
              "      <td>0.316289</td>\n",
              "      <td>0.410714</td>\n",
              "      <td>-1.092828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pedir</td>\n",
              "      <td>-0.642872</td>\n",
              "      <td>1.114552</td>\n",
              "      <td>0.298639</td>\n",
              "      <td>0.830782</td>\n",
              "      <td>-1.192118</td>\n",
              "      <td>0.899174</td>\n",
              "      <td>0.342365</td>\n",
              "      <td>-0.214228</td>\n",
              "      <td>1.042447</td>\n",
              "      <td>0.396128</td>\n",
              "      <td>1.225741</td>\n",
              "      <td>-0.756759</td>\n",
              "      <td>-0.969425</td>\n",
              "      <td>-1.216452</td>\n",
              "      <td>0.714174</td>\n",
              "      <td>-0.135105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1645</th>\n",
              "      <td>negra</td>\n",
              "      <td>0.587953</td>\n",
              "      <td>-1.432679</td>\n",
              "      <td>-2.887429</td>\n",
              "      <td>0.356008</td>\n",
              "      <td>-1.186241</td>\n",
              "      <td>-0.938067</td>\n",
              "      <td>-1.311168</td>\n",
              "      <td>-0.640874</td>\n",
              "      <td>0.838862</td>\n",
              "      <td>-0.254578</td>\n",
              "      <td>-0.155816</td>\n",
              "      <td>-0.753237</td>\n",
              "      <td>1.346732</td>\n",
              "      <td>0.426177</td>\n",
              "      <td>0.302166</td>\n",
              "      <td>-0.673310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>supervisor</td>\n",
              "      <td>-0.494749</td>\n",
              "      <td>-0.726946</td>\n",
              "      <td>1.601808</td>\n",
              "      <td>0.693650</td>\n",
              "      <td>-0.501509</td>\n",
              "      <td>0.358264</td>\n",
              "      <td>0.963258</td>\n",
              "      <td>-0.906863</td>\n",
              "      <td>0.321863</td>\n",
              "      <td>-1.755081</td>\n",
              "      <td>0.725751</td>\n",
              "      <td>-1.163746</td>\n",
              "      <td>0.417186</td>\n",
              "      <td>-0.497902</td>\n",
              "      <td>-0.984042</td>\n",
              "      <td>0.393289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>público</td>\n",
              "      <td>-0.495212</td>\n",
              "      <td>0.494785</td>\n",
              "      <td>-0.608081</td>\n",
              "      <td>0.365968</td>\n",
              "      <td>0.210625</td>\n",
              "      <td>-1.317109</td>\n",
              "      <td>2.777832</td>\n",
              "      <td>-0.687007</td>\n",
              "      <td>0.332343</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.622183</td>\n",
              "      <td>0.478485</td>\n",
              "      <td>-0.123267</td>\n",
              "      <td>-1.051064</td>\n",
              "      <td>0.128852</td>\n",
              "      <td>0.542518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1648</th>\n",
              "      <td>mídia</td>\n",
              "      <td>-1.130954</td>\n",
              "      <td>0.261521</td>\n",
              "      <td>-0.089227</td>\n",
              "      <td>0.380828</td>\n",
              "      <td>0.122371</td>\n",
              "      <td>0.573752</td>\n",
              "      <td>-0.780007</td>\n",
              "      <td>0.077798</td>\n",
              "      <td>-1.779162</td>\n",
              "      <td>-1.129784</td>\n",
              "      <td>1.092693</td>\n",
              "      <td>2.512996</td>\n",
              "      <td>-0.797869</td>\n",
              "      <td>1.292808</td>\n",
              "      <td>1.181169</td>\n",
              "      <td>-0.491366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>732</td>\n",
              "      <td>-1.358909</td>\n",
              "      <td>-0.804184</td>\n",
              "      <td>-1.173582</td>\n",
              "      <td>0.079478</td>\n",
              "      <td>0.208246</td>\n",
              "      <td>1.522391</td>\n",
              "      <td>0.704800</td>\n",
              "      <td>-2.306075</td>\n",
              "      <td>-0.013399</td>\n",
              "      <td>1.466617</td>\n",
              "      <td>0.825155</td>\n",
              "      <td>-2.111932</td>\n",
              "      <td>-0.420072</td>\n",
              "      <td>1.030042</td>\n",
              "      <td>-1.188781</td>\n",
              "      <td>-1.264911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1650 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         palavra        x1        x2  ...       x14       x15       x16\n",
              "0                 1.255359 -0.520802  ... -1.117128  0.657504 -0.455104\n",
              "1          reter -0.066469 -0.606716  ...  1.331327 -1.361109 -0.763709\n",
              "2      infomoney -0.112089 -0.019657  ...  0.317617 -0.312168  0.463600\n",
              "3           cair -0.276426  0.004193  ...  0.316289  0.410714 -1.092828\n",
              "4          pedir -0.642872  1.114552  ... -1.216452  0.714174 -0.135105\n",
              "...          ...       ...       ...  ...       ...       ...       ...\n",
              "1645       negra  0.587953 -1.432679  ...  0.426177  0.302166 -0.673310\n",
              "1646  supervisor -0.494749 -0.726946  ... -0.497902 -0.984042  0.393289\n",
              "1647     público -0.495212  0.494785  ... -1.051064  0.128852  0.542518\n",
              "1648       mídia -1.130954  0.261521  ...  1.292808  1.181169 -0.491366\n",
              "1649         732 -1.358909 -0.804184  ...  1.030042 -1.188781 -1.264911\n",
              "\n",
              "[1650 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G15JGrP4f2se",
        "colab_type": "text"
      },
      "source": [
        "Nessa ultima etapa, sera exportado as palavras e seus respectivos vetores, divididas em dois arquivos. Para construir a **view**, optei em usar o Embedding Projector do TensorFlow, http://projector.tensorflow.org. Basta clicar no botão load e selecionar os dois arquivos que foram exportados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkVa7ld5H4OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for j in palavras:\n",
        "  out_m.write(j + \"\\n\")\n",
        "for ix1 in range(linhas):\n",
        "  for ix2 in range(colunas):\n",
        "    out_v.write(str((vectors[ix1,ix2])) + \"\\t\")\n",
        "  out_v.write(str('\\n'))\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Briefing",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dauberson/hello-world/blob/master/Briefing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZHS6Y0wT5Pq",
        "colab_type": "text"
      },
      "source": [
        "Para realiza o briefing eu optei usar o Word2Vec, é uma técnica de processamento de linguagem natural(NLP) que tem a ideia de transformar uma palavra em um vetor númerico que consiga o representar semanticamente, cada palavra tem uma unica representação. Para fazer essa representação podemos usar o encoding, porem esse metodo não leva em consideração a similaridade entre as palavras. Mas temos uma solução para isso, embedding; Esse metodo consegue considerar a similaridade entre as palavras. Word2Vec usa o metodo de embedding e a similaridade entre as palavras vem atraves de palavras \"vizinhas\". Então para prosseguir com esse metodo, o nosso banco de dados foi preenchido com frases. \n",
        "Dado as definições acimas, usamos o Skip Gram para definir esses \"vizinhos\", para essa tecnica foi necessario definir um raio de vizinhança, conhecido como windows size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dypLcdjrKzsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ['king is a strong man', \n",
        "          'queen is a wise woman', \n",
        "          'boy is a young man',\n",
        "          'girl is a young woman',\n",
        "          'prince is a young king',\n",
        "          'princess is a young queen',\n",
        "          'man is strong', \n",
        "          'woman is pretty',\n",
        "          'prince is a boy will be king',\n",
        "          'princess is a girl will be queen']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0zpeV3xUSWD",
        "colab_type": "text"
      },
      "source": [
        "Função que remove as palavras que redundantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEO3wJgJK74p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(corpus): \n",
        "    stop_words = ['is', 'a', 'will', 'be']\n",
        "    results = []\n",
        "    for text in corpus:\n",
        "        tmp = text.split(' ')\n",
        "        for stop_word in stop_words:\n",
        "            if stop_word in tmp:\n",
        "                tmp.remove(stop_word)\n",
        "        results.append(\" \".join(tmp))\n",
        "    \n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar8LexNuMWpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = remove_stop_words(data) #aplicando a função em cima do banco de dados"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64cp8KjTYbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "palavras = []\n",
        "for texto in data:\n",
        "    for palavra in texto.split(' '):\n",
        "        palavras.append(palavra)\n",
        "\n",
        "palavras = set(palavras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcuu8PlJVdtD",
        "colab_type": "text"
      },
      "source": [
        "Skip Gram em ação! windows siza = 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2qp1n_HVUnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2int = {}\n",
        "\n",
        "for i,palavra in enumerate(palavras):\n",
        "    word2int[palavra] = i\n",
        "\n",
        "sentencas = []\n",
        "for sentenca in data:\n",
        "    sentencas.append(sentenca.split())\n",
        "    \n",
        "WINDOW_SIZE = 2\n",
        "\n",
        "data2 = []\n",
        "for sentenca in sentencas:\n",
        "    for idx, palavra in enumerate(sentenca):\n",
        "        for vizinho in sentenca[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentenca)) + 1] : \n",
        "            if vizinho != palavra:\n",
        "                data2.append([palavra, vizinho])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuqkWCH1WVqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data2, columns = ['palavra', 'vizinho'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLZrAItiWW6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "914821cb-391a-44b9-b6a5-965e72721964"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>palavra</th>\n",
              "      <th>vizinho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>king</td>\n",
              "      <td>strong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>king</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>strong</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>strong</td>\n",
              "      <td>man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>man</td>\n",
              "      <td>king</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>man</td>\n",
              "      <td>strong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>queen</td>\n",
              "      <td>wise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>queen</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>wise</td>\n",
              "      <td>queen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wise</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  palavra vizinho\n",
              "0    king  strong\n",
              "1    king     man\n",
              "2  strong    king\n",
              "3  strong     man\n",
              "4     man    king\n",
              "5     man  strong\n",
              "6   queen    wise\n",
              "7   queen   woman\n",
              "8    wise   queen\n",
              "9    wise   woman"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm3MsnpMYaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "ONE_HOT_DIM = len(palavras)\n",
        "\n",
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot_encoding(data_point_index):\n",
        "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
        "    one_hot_encoding[data_point_index] = 1\n",
        "    return one_hot_encoding\n",
        "\n",
        "X = [] # input word\n",
        "Y = [] # target word\n",
        "\n",
        "for x, y in zip(df['palavra'], df['vizinho']):\n",
        "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
        "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
        "\n",
        "# convert them to numpy arrays\n",
        "X_train = np.asarray(X)\n",
        "Y_train = np.asarray(Y)\n",
        "\n",
        "# making placeholders for X_train and Y_train\n",
        "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
        "\n",
        "# word embedding will be 2 dimension for 2d visualization\n",
        "EMBEDDING_DIM = 2 \n",
        "\n",
        "# hidden layer: which represents word vector eventually\n",
        "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
        "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
        "\n",
        "# output layer\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
        "b2 = tf.Variable(tf.random_normal([1]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
        "\n",
        "# loss function: cross entropy\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
        "\n",
        "# training operation\n",
        "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TTsqbrLZOdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f5d30f9-0381-4d21-b3a2-3a6be0e8ebf0"
      },
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) \n",
        "\n",
        "iteration = 50000\n",
        "for i in range(iteration):\n",
        "    # input is X_train which is one hot encoded word\n",
        "    # label is Y_train which is one hot encoded neighbor word\n",
        "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
        "    if i % 3000 == 0:\n",
        "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 loss is :  4.8001804\n",
            "iteration 3000 loss is :  1.7833486\n",
            "iteration 6000 loss is :  1.7253203\n",
            "iteration 9000 loss is :  1.7061269\n",
            "iteration 12000 loss is :  1.693076\n",
            "iteration 15000 loss is :  1.681834\n",
            "iteration 18000 loss is :  1.672219\n",
            "iteration 21000 loss is :  1.6648515\n",
            "iteration 24000 loss is :  1.6596179\n",
            "iteration 27000 loss is :  1.6557524\n",
            "iteration 30000 loss is :  1.6526197\n",
            "iteration 33000 loss is :  1.6499487\n",
            "iteration 36000 loss is :  1.6476188\n",
            "iteration 39000 loss is :  1.6455579\n",
            "iteration 42000 loss is :  1.6437148\n",
            "iteration 45000 loss is :  1.6420516\n",
            "iteration 48000 loss is :  1.6405396\n",
            "iteration 51000 loss is :  1.6391562\n",
            "iteration 54000 loss is :  1.637883\n",
            "iteration 57000 loss is :  1.6367056\n",
            "iteration 60000 loss is :  nan\n",
            "iteration 63000 loss is :  nan\n",
            "iteration 66000 loss is :  nan\n",
            "iteration 69000 loss is :  nan\n",
            "iteration 72000 loss is :  nan\n",
            "iteration 75000 loss is :  nan\n",
            "iteration 78000 loss is :  nan\n",
            "iteration 81000 loss is :  nan\n",
            "iteration 84000 loss is :  nan\n",
            "iteration 87000 loss is :  nan\n",
            "iteration 90000 loss is :  nan\n",
            "iteration 93000 loss is :  nan\n",
            "iteration 96000 loss is :  nan\n",
            "iteration 99000 loss is :  nan\n",
            "iteration 102000 loss is :  nan\n",
            "iteration 105000 loss is :  nan\n",
            "iteration 108000 loss is :  nan\n",
            "iteration 111000 loss is :  nan\n",
            "iteration 114000 loss is :  nan\n",
            "iteration 117000 loss is :  nan\n",
            "iteration 120000 loss is :  nan\n",
            "iteration 123000 loss is :  nan\n",
            "iteration 126000 loss is :  nan\n",
            "iteration 129000 loss is :  nan\n",
            "iteration 132000 loss is :  nan\n",
            "iteration 135000 loss is :  nan\n",
            "iteration 138000 loss is :  nan\n",
            "iteration 141000 loss is :  nan\n",
            "iteration 144000 loss is :  nan\n",
            "iteration 147000 loss is :  nan\n",
            "iteration 150000 loss is :  nan\n",
            "iteration 153000 loss is :  nan\n",
            "iteration 156000 loss is :  nan\n",
            "iteration 159000 loss is :  nan\n",
            "iteration 162000 loss is :  nan\n",
            "iteration 165000 loss is :  nan\n",
            "iteration 168000 loss is :  nan\n",
            "iteration 171000 loss is :  nan\n",
            "iteration 174000 loss is :  nan\n",
            "iteration 177000 loss is :  nan\n",
            "iteration 180000 loss is :  nan\n",
            "iteration 183000 loss is :  nan\n",
            "iteration 186000 loss is :  nan\n",
            "iteration 189000 loss is :  nan\n",
            "iteration 192000 loss is :  nan\n",
            "iteration 195000 loss is :  nan\n",
            "iteration 198000 loss is :  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa7PARWCZYFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e1a24216-7c8e-4a69-cd4d-b3a7bd01a318"
      },
      "source": [
        "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
        "vectors = sess.run(W1 + b1)\n",
        "print(vectors)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]\n",
            " [nan nan]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYo2qFBuZY55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "5110e478-5e8d-4946-d5ce-9a5cdce6e7e7"
      },
      "source": [
        "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
        "w2v_df['palavra'] = palavras\n",
        "w2v_df = w2v_df[['palavra', 'x1', 'x2']]\n",
        "w2v_df"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>palavra</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>woman</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>man</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>princess</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>king</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>girl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>prince</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>wise</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>young</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>strong</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>pretty</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>boy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>queen</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     palavra  x1  x2\n",
              "0      woman NaN NaN\n",
              "1        man NaN NaN\n",
              "2   princess NaN NaN\n",
              "3       king NaN NaN\n",
              "4       girl NaN NaN\n",
              "5     prince NaN NaN\n",
              "6       wise NaN NaN\n",
              "7      young NaN NaN\n",
              "8     strong NaN NaN\n",
              "9     pretty NaN NaN\n",
              "10       boy NaN NaN\n",
              "11     queen NaN NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s01RP8g1Zry2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "32e7267f-cb33-4480-b7a5-91111422c0a2"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for word, x1, x2 in zip(w2v_df['palavra'], w2v_df['x1'], w2v_df['x2']):\n",
        "    ax.annotate(word, (x1,x2 ))\n",
        "    \n",
        "PADDING = 1.0\n",
        "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
        "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
        "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
        "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
        " \n",
        "plt.xlim(x_axis_min,x_axis_max)\n",
        "plt.ylim(y_axis_min,y_axis_max)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-ffae12dbb75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_axis_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPADDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_axis_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_axis_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_axis_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_axis_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"figure.figsize\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mxlim\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mset_xlim\u001b[0;34m(self, left, right, emit, auto, xmin, xmax)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_converted_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_converted_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_validate_converted_limits\u001b[0;34m(self, limit, convert)\u001b[0m\n\u001b[1;32m   3137\u001b[0m             if (isinstance(converted_limit, Real)\n\u001b[1;32m   3138\u001b[0m                     and not np.isfinite(converted_limit)):\n\u001b[0;32m-> 3139\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Axis limits cannot be NaN or Inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Axis limits cannot be NaN or Inf"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAJDCAYAAAA8QNGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFIJJREFUeJzt3V+I5Xd5x/HPY2IqaLTQbEGyiQl0\nU01ViB3SFC8MmJYkF5sLW0lArBLcm0ZsFSGiqMQrlVoQ4p8tlVRB0+iFLLiSgo0ExEhWbINJiCzR\nmo1CosbcBI1pn17MKONkd+dkcp7ZPcnrBQvz+53vnPPAl9l97++cOae6OwAAzHjBqR4AAOC5TGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAM2ja2qupzVfVIVX3/BLdXVX2yqo5W1T1V9brljwkAsJoWubJ1\nS5IrT3L7VUn2bfw5kOTTz34sAIDnhm1jq7vvTPKLkyy5Jsnne91dSf6wql6+rAEBAFbZMl6zdW6S\nhzYdH9s4BwDwvHfmbj5YVR3I+lONefGLX/znr3zlK3fz4QEAduS73/3uz7p7z06+dxmx9XCS8zYd\n79049zTdfTDJwSRZW1vrI0eOLOHhAQBmVdX/7PR7l/E04qEkb934rcTLkjze3T9dwv0CAKy8ba9s\nVdWXklye5JyqOpbkQ0lemCTd/Zkkh5NcneRokieSvH1qWACAVbNtbHX3ddvc3kn+fmkTAQA8h3gH\neQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJ\nLQCAQWILAGCQ2AIAGCS2AAAGLRRbVXVlVT1QVUer6sbj3H5+Vd1RVd+rqnuq6urljwoAsHq2ja2q\nOiPJzUmuSnJxkuuq6uItyz6Q5LbuviTJtUk+texBAQBW0SJXti5NcrS7H+zuJ5PcmuSaLWs6yUs3\nvn5Zkp8sb0QAgNV15gJrzk3y0KbjY0n+YsuaDyf5j6p6Z5IXJ7liKdMBAKy4Zb1A/rokt3T33iRX\nJ/lCVT3tvqvqQFUdqaojjz766JIeGgDg9LVIbD2c5LxNx3s3zm12fZLbkqS7v53kRUnO2XpH3X2w\nu9e6e23Pnj07mxgAYIUsElt3J9lXVRdW1VlZfwH8oS1rfpzkjUlSVa/Kemy5dAUAPO9tG1vd/VSS\nG5LcnuT+rP/W4b1VdVNV7d9Y9p4k76iq/07ypSRv6+6eGhoAYFUs8gL5dPfhJIe3nPvgpq/vS/L6\n5Y4GALD6vIM8AMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwB\nAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIMWiq2qurKqHqiqo1V14wnWvLmq7quqe6vqi8sdEwBg\nNZ253YKqOiPJzUn+KsmxJHdX1aHuvm/Tmn1J3pfk9d39WFX98dTAAACrZJErW5cmOdrdD3b3k0lu\nTXLNljXvSHJzdz+WJN39yHLHBABYTYvE1rlJHtp0fGzj3GYXJbmoqr5VVXdV1ZXLGhAAYJVt+zTi\nM7iffUkuT7I3yZ1V9Zru/uXmRVV1IMmBJDn//POX9NAAAKevRa5sPZzkvE3HezfObXYsyaHu/k13\n/zDJD7IeX7+nuw9291p3r+3Zs2enMwMArIxFYuvuJPuq6sKqOivJtUkObVnz1axf1UpVnZP1pxUf\nXOKcAAAradvY6u6nktyQ5PYk9ye5rbvvraqbqmr/xrLbk/y8qu5LckeS93b3z6eGBgBYFdXdp+SB\n19bW+siRI6fksQEAnomq+m53r+3ke72DPADAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACDxBYAwCCxBQAwSGwBAAwS\nWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADFootqrqyqp6oKqOVtWNJ1n3pqrq\nqlpb3ogAAKtr29iqqjOS3JzkqiQXJ7muqi4+zrqzk7wryXeWPSQAwKpa5MrWpUmOdveD3f1kkluT\nXHOcdR9J8tEkv1rifAAAK22R2Do3yUObjo9tnPudqnpdkvO6+2tLnA0AYOU96xfIV9ULknwiyXsW\nWHugqo5U1ZFHH3302T40AMBpb5HYejjJeZuO926c+62zk7w6yTer6kdJLkty6Hgvku/ug9291t1r\ne/bs2fnUAAArYpHYujvJvqq6sKrOSnJtkkO/vbG7H+/uc7r7gu6+IMldSfZ395GRiQEAVsi2sdXd\nTyW5IcntSe5Pclt331tVN1XV/ukBAQBW2ZmLLOruw0kObzn3wROsvfzZjwUA8NzgHeQBAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBBYgsAYJDYAgAYJLYAAAaJLQCA\nQWILAGCQ2AIAGLRQbFXVlVX1QFUdraobj3P7u6vqvqq6p6q+UVWvWP6oAACrZ9vYqqozktyc5Kok\nFye5rqou3rLse0nWuvu1Sb6S5GPLHhQAYBUtcmXr0iRHu/vB7n4yya1Jrtm8oLvv6O4nNg7vSrJ3\nuWMCAKymRWLr3CQPbTo+tnHuRK5P8vVnMxQAwHPFmcu8s6p6S5K1JG84we0HkhxIkvPPP3+ZDw0A\ncFpa5MrWw0nO23S8d+Pc76mqK5K8P8n+7v718e6ouw9291p3r+3Zs2cn8wIArJRFYuvuJPuq6sKq\nOivJtUkObV5QVZck+WzWQ+uR5Y8JALCato2t7n4qyQ1Jbk9yf5LbuvveqrqpqvZvLPt4kpck+XJV\n/VdVHTrB3QEAPK8s9Jqt7j6c5PCWcx/c9PUVS54LAOA5wTvIAwAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEA\nDBJbAACDxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBoodiq\nqiur6oGqOlpVNx7n9j+oqn/fuP07VXXBsgcFAFhF28ZWVZ2R5OYkVyW5OMl1VXXxlmXXJ3msu/8k\nyT8n+eiyBwUAWEWLXNm6NMnR7n6wu59McmuSa7asuSbJv218/ZUkb6yqWt6YAACraZHYOjfJQ5uO\nj22cO+6a7n4qyeNJ/mgZAwIArLIzd/PBqupAkgMbh7+uqu/v5uOzVOck+dmpHoIdsXerzf6tLnu3\n2v50p9+4SGw9nOS8Tcd7N84db82xqjozycuS/HzrHXX3wSQHk6SqjnT32k6G5tSzf6vL3q02+7e6\n7N1qq6ojO/3eRZ5GvDvJvqq6sKrOSnJtkkNb1hxK8ncbX/9Nkv/s7t7pUAAAzxXbXtnq7qeq6oYk\ntyc5I8nnuvveqropyZHuPpTkX5N8oaqOJvlF1oMMAOB5b6HXbHX34SSHt5z74Kavf5Xkb5/hYx98\nhus5vdi/1WXvVpv9W132brXteP/Ks30AAHN8XA8AwKDx2PJRP6trgb17d1XdV1X3VNU3quoVp2JO\njm+7/du07k1V1VXlt6ROI4vsX1W9eeNn8N6q+uJuz8jxLfB35/lVdUdVfW/j78+rT8WcPF1Vfa6q\nHjnRW1PVuk9u7O09VfW6Re53NLZ81M/qWnDvvpdkrbtfm/VPDvjY7k7JiSy4f6mqs5O8K8l3dndC\nTmaR/auqfUnel+T13f1nSf5h1wflaRb82ftAktu6+5Ks/0LZp3Z3Sk7iliRXnuT2q5Ls2/hzIMmn\nF7nT6StbPupndW27d919R3c/sXF4V9bfg43TwyI/e0nykaz/B+dXuzkc21pk/96R5ObufixJuvuR\nXZ6R41tk7zrJSze+flmSn+zifJxEd9+Z9XdVOJFrkny+192V5A+r6uXb3e90bPmon9W1yN5tdn2S\nr49OxDOx7f5tXP4+r7u/tpuDsZBFfv4uSnJRVX2rqu6qqpP9b5zds8jefTjJW6rqWNZ/0/+duzMa\nS/BM/21Msssf18NzU1W9Jclakjec6llYTFW9IMknkrztFI/Czp2Z9acyLs/6VeU7q+o13f3LUzoV\ni7guyS3d/U9V9ZdZf5/KV3f3/53qwZgxfWXrmXzUT072UT/sukX2LlV1RZL3J9nf3b/epdnY3nb7\nd3aSVyf5ZlX9KMllSQ55kfxpY5Gfv2NJDnX3b7r7h0l+kPX44tRaZO+uT3JbknT3t5O8KOufm8jp\nb6F/G7eaji0f9bO6tt27qrokyWezHlpeL3J6Oen+dffj3X1Od1/Q3Rdk/TV3+7t7x5/9xVIt8nfn\nV7N+VStVdU7Wn1Z8cDeH5LgW2bsfJ3ljklTVq7IeW4/u6pTs1KEkb934rcTLkjze3T/d7ptGn0b0\nUT+ra8G9+3iSlyT58sbvNPy4u/efsqH5nQX3j9PUgvt3e5K/rqr7kvxvkvd2t2cFTrEF9+49Sf6l\nqv4x6y+Wf5uLDKeHqvpS1v8Tc87Ga+o+lOSFSdLdn8n6a+yuTnI0yRNJ3r7Q/dpfAIA53kEeAGCQ\n2AIAGCS2AAAGiS0AgEFiCwBgkNgCABgktgAABoktAIBB/w8d9K5C3G8K/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}